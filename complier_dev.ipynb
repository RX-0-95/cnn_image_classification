{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd01e176acbd42c1e3e7f101af43ec62a5c4debfebb92accde047567277d89b2ce2",
   "display_name": "Python 3.8.10 64-bit ('tf-gpu': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,) int64\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "import gzip, shutil\n",
    "import os \n",
    "import data_utils as du \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "import numpy as np \n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib.pyplot import imread\n",
    "import matplotlib.pyplot as plt \n",
    "import platform\n",
    "import data_utils as du \n",
    "from cnn_model import *\n",
    "from solver import Solver\n",
    "import qunatize_resnet20 as q_res\n",
    "import quantize_layers as ql\n",
    "import tensorflow_hub as hub \n",
    "import quantize_util as qu \n",
    "import quantize_complier as qc \n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "plt.rcParams['figure.figsize'] = (15.0, 15.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "Data_folder = 'CIFAR10_Data'\n",
    "Data_fn = 'cifar-10-python'\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = du.get_CIFAR10_data(subtract_mean =True)\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<quantize_layers.quantizeConv2D object at 0x7f97c85c3940>\n<quantize_layers.quantizeConv2D object at 0x7f97c85c5af0>\n<quantize_layers.quantizeConv2D object at 0x7f97c85c3b50>\n<quantize_layers.quantizeConv2D object at 0x7f98206ea640>\n<quantize_layers.quantizeConv2D object at 0x7f98206c72b0>\n<quantize_layers.quantizeConv2D object at 0x7f97c85ee520>\n<quantize_layers.quantizeConv2D object at 0x7f97c85ee9a0>\n<quantize_layers.quantizeConv2D object at 0x7f98206ea2e0>\n<quantize_layers.quantizeConv2D object at 0x7f98206f4460>\n<quantize_layers.quantizeConv2D object at 0x7f9820609e80>\n<quantize_layers.quantizeConv2D object at 0x7f98206ce070>\n<quantize_layers.quantizeConv2D object at 0x7f982063b610>\n<quantize_layers.quantizeConv2D object at 0x7f98205ff3a0>\n<quantize_layers.quantizeConv2D object at 0x7f982060abb0>\n<quantize_layers.quantizeConv2D object at 0x7f97c812ee20>\n<quantize_layers.quantizeConv2D object at 0x7f97c8106670>\n<quantize_layers.quantizeConv2D object at 0x7f97c80ffd60>\n<quantize_layers.quantizeConv2D object at 0x7f97c81134c0>\n<quantize_layers.quantizeConv2D object at 0x7f983dc1e940>\n<quantize_layers.quantizeDense object at 0x7f983dc35e50>\n"
     ]
    }
   ],
   "source": [
    "data_set = {\n",
    "    'train_data': X_train,\n",
    "    'train_label': y_train,\n",
    "    'val_data':X_val,\n",
    "    'val_label':y_val,\n",
    "}\n",
    "train_options = {\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 0.001,\n",
    "    'epoch_num': 1,\n",
    "    'from_logits':True,\n",
    "    'save_dir': 'resnet20_model'\n",
    "}\n",
    "model_path = 'resnet20_model/trained_15'\n",
    "full_model = hub.KerasLayer(model_path,trainable=True)\n",
    "\n",
    "qunatizer = ql.ModelQunatize() \n",
    "model_q = q_res.Resnet20(qunatizer)\n",
    "qu.copy_weight(model_q,full_model)\n",
    "q_layers = model_q.get_quantizable_layers()\n",
    "for layer in q_layers:\n",
    "    print(layer)\n",
    "\n",
    "solver = Solver(model_q,data_set,train_options,save_model=False)\n",
    "\n",
    "data_shuffler = qc.DataShuffler(X_train, y_train,sample_size=1024)\n",
    "\n",
    "\n",
    "def train_fn(model,solver=solver):\n",
    "    solver = Solver(model,data_set,train_options,\n",
    "                    save_model=False,verbose=False,\n",
    "                    plot_graph=False)\n",
    "    return solver.train(iter=int(5000/32))\n",
    "\n",
    "def test_fn(model,solver=solver,shuffle=False):\n",
    "    solver = Solver(model,data_set,save_model=False,\n",
    "                    verbose=False,plot_graph=False)\n",
    "    #X_s,y_s = data_shuffler.sample(shuffle)\n",
    "    return solver.test_model(X_test[0:1024-1],y_test[0:1024-1])\n",
    "    #return solver.test_model(X_test[0:1024*6-1],y_test[0:1024*6-1])\n",
    "    #return solver.test_model(X_s,y_s)\n",
    "    #return solver.test_model(X_test,y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Full percision model accuracy: 0.7077223658561707\n",
      "************Coarse search begin************\n",
      "Coarse seach done, compression rate:(0.25, 2146768),quantize model accuracy: 0.7028347849845886\n",
      "\n",
      "************Fine search begin************\n",
      "\n",
      "In compression search+++++++++++++++++++++++++++\n",
      "reduce wl on layer 15 kernel,acc_loss: 0.00782012939453125, current accuracy:0.6999022364616394, compression rate: (0.24570703494741863, 2109904)\n",
      "<quantize_complier.WlflList object at 0x7f9884ce6580>[[8,7,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][7,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,8,6]]\n",
      "Compression rate:0.24570703494741863\n",
      "Traget Compression rate:0.12\n",
      "------Fine tune the model------\n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.8321 - train_loss: 0.4679 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9050 - train_loss: 0.2791 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9281 - train_loss: 0.2037 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9421 - train_loss: 0.1794 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9413 - train_loss: 0.1798 - val_acc: NA    - val_loss: NA   \n",
      "In compression search+++++++++++++++++++++++++++\n",
      "reduce wl on layer 14 kernel,acc_loss: 0.00782012939453125, current accuracy:0.6891495585441589, compression rate: (0.24141406989483727, 2073040)\n",
      "<quantize_complier.WlflList object at 0x7f9884ce6580>[[8,7,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][7,8,0,0][7,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,8,6]]\n",
      "Compression rate:0.24141406989483727\n",
      "Traget Compression rate:0.12\n",
      "------Fine tune the model------\n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9561 - train_loss: 0.1285 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9503 - train_loss: 0.1444 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9603 - train_loss: 0.1234 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9593 - train_loss: 0.1332 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9627 - train_loss: 0.1194 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9619 - train_loss: 0.1252 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9679 - train_loss: 0.1080 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9575 - train_loss: 0.1356 - val_acc: NA    - val_loss: NA   \n",
      "!!!!!Can't Compress Any More!\n",
      "Start loss search++++++++++++++++++++++++++++\n",
      "reduce wl on layer 19 bias,acc_loss: -0.0029325485229492188, current accuracy:0.6764418482780457, compression rate: (0.24570587040611747, 2109894)\n",
      "<quantize_complier.WlflList object at 0x7f9884ce6580>[[8,7,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,9,0,0][7,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,7,6]]\n",
      "<quantize_complier.AccuracyLossPerBitList object at 0x7f9885227460>[[2.2627689304499654e-06,None][-1.6970766409940552e-06,None][-1.2728074807455414e-06,None][-2.1213459149294067e-06,None][-8.485383204970276e-07,None][-4.242691602485138e-07,None][-1.6970766409940552e-06,None][-1.0606729574647034e-06,None][-5.303364787323517e-07,None][-1.0606729006212845e-07,None][-1.0606729006212845e-07,None][-3.1820187018638535e-07,None][-1.0606729006212845e-07,None][-1.0606729006212845e-07,None][-5.3033645031064225e-08,None][-1.0606729006212845e-07,None][-1.3258411968308792e-07,None][-7.955046754659634e-08,None][-5.3033645031064225e-08,None][-1.5273690223693848e-06,-0.0002932548522949219]]\n",
      "reduce wl on layer 19 bias,acc_loss: -0.0029325485229492188, current accuracy:0.6764418482780457, compression rate: (0.24570470586481633, 2109884)\n",
      "<quantize_complier.WlflList object at 0x7f9884ce6580>[[8,7,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,9,0,0][7,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,6,5]]\n",
      "<quantize_complier.AccuracyLossPerBitList object at 0x7f9885227460>[[0.0,None][-1.6970766409940552e-06,None][-4.242691602485138e-07,None][-1.6970766409940552e-06,None][-8.485383204970276e-07,None][-4.242691602485138e-07,None][-1.2728074807455414e-06,None][-1.0606729574647034e-06,None][-5.303364787323517e-07,None][-1.0606729006212845e-07,None][-1.0606729006212845e-07,None][-3.1820187018638535e-07,None][-1.0606729006212845e-07,None][-1.0606729006212845e-07,None][-5.3033645031064225e-08,None][-1.0606729006212845e-07,None][-1.0606729006212845e-07,None][-5.3033645031064225e-08,None][-5.3033645031064225e-08,None][-1.5273690223693848e-06,-0.0002932548522949219]]\n",
      "reduce wl on layer 19 bias,acc_loss: -0.0019550323486328125, current accuracy:0.6754643321037292, compression rate: (0.24570354132351516, 2109874)\n",
      "<quantize_complier.WlflList object at 0x7f9884ce6580>[[8,7,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,9,0,0][7,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,5,4]]\n",
      "<quantize_complier.AccuracyLossPerBitList object at 0x7f9885227460>[[2.2627689304499654e-06,None][-2.1213459149294067e-06,None][0.0,None][-2.1213459149294067e-06,None][-8.485383204970276e-07,None][-8.485383204970276e-07,None][-1.6970766409940552e-06,None][-1.0606729574647034e-06,None][-3.1820187018638535e-07,None][-2.121345801242569e-07,None][-1.0606729006212845e-07,None][-2.121345801242569e-07,None][-3.1820187018638535e-07,None][-1.0606729006212845e-07,None][-7.955046754659634e-08,None][-1.0606729006212845e-07,None][-1.0606729006212845e-07,None][-5.3033645031064225e-08,None][-5.3033645031064225e-08,None][-1.5273690223693848e-06,-0.00019550323486328125]]\n",
      "reduce wl on layer 19 bias,acc_loss: -0.0019550323486328125, current accuracy:0.6754643321037292, compression rate: (0.24570237678221402, 2109864)\n",
      "<quantize_complier.WlflList object at 0x7f9884ce6580>[[8,7,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,9,0,0][7,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,4,3]]\n",
      "<quantize_complier.AccuracyLossPerBitList object at 0x7f9885227460>[[2.2627689304499654e-06,None][-1.6970766409940552e-06,None][-4.242691602485138e-07,None][-1.2728074807455414e-06,None][-8.485383204970276e-07,None][-4.242691602485138e-07,None][-1.2728074807455414e-06,None][-1.0606729574647034e-06,None][-5.303364787323517e-07,None][-1.0606729006212845e-07,None][-1.0606729006212845e-07,None][-3.1820187018638535e-07,None][-1.0606729006212845e-07,None][-1.0606729006212845e-07,None][-5.3033645031064225e-08,None][-1.0606729006212845e-07,None][-1.0606729006212845e-07,None][-1.0606729006212845e-07,None][-2.6516822515532112e-08,None][-1.5273690223693848e-06,-0.00019550323486328125]]\n",
      "reduce wl on layer 19 bias,acc_loss: -0.0029325485229492188, current accuracy:0.6764418482780457, compression rate: (0.24570121224091285, 2109854)\n",
      "<quantize_complier.WlflList object at 0x7f9884ce6580>[[8,7,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,9,0,0][7,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,3,2]]\n",
      "<quantize_complier.AccuracyLossPerBitList object at 0x7f9885227460>[[2.2627689304499654e-06,None][-1.6970766409940552e-06,None][-4.242691602485138e-07,None][-1.6970766409940552e-06,None][-4.242691602485138e-07,None][0.0,None][-1.2728074807455414e-06,None][-1.0606729574647034e-06,None][-2.121345801242569e-07,None][-3.1820187018638535e-07,None][0.0,None][-1.0606729006212845e-07,None][-2.121345801242569e-07,None][0.0,None][-7.955046754659634e-08,None][-7.955046754659634e-08,None][-7.955046754659634e-08,None][-5.3033645031064225e-08,None][2.6516822515532112e-08,None][-4.582107067108154e-06,-0.0002932548522949219]]\n",
      "reduce wl on layer 19 bias,acc_loss: -0.003910064697265625, current accuracy:0.6774193644523621, compression rate: (0.2457000476996117, 2109844)\n",
      "<quantize_complier.WlflList object at 0x7f9884ce6580>[[8,7,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,9,0,0][7,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,2,1]]\n",
      "<quantize_complier.AccuracyLossPerBitList object at 0x7f9885227460>[[0.0,None][-1.6970766409940552e-06,None][-8.485383204970276e-07,None][-4.242691602485138e-07,None][-8.485383204970276e-07,None][0.0,None][-8.485383204970276e-07,None][-1.2728074807455414e-06,None][-2.121345801242569e-07,None][-2.121345801242569e-07,None][0.0,None][-2.121345801242569e-07,None][-1.0606729006212845e-07,None][5.3033645031064225e-08,None][-1.0606729006212845e-07,None][-7.955046754659634e-08,None][-2.6516822515532112e-08,None][-2.6516822515532112e-08,None][0.0,None][-1.5273690223693848e-06,-0.0003910064697265625]]\n",
      "reduce wl on layer 19 kernel,acc_loss: 0.016617774963378906, current accuracy:0.6568915247917175, compression rate: (0.2456255170563377, 2109204)\n",
      "<quantize_complier.WlflList object at 0x7f9884ce6580>[[8,7,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][8,9,0,0][7,8,0,0][8,8,0,0][8,8,0,0][8,8,0,0][7,8,2,1]]\n",
      "<quantize_complier.AccuracyLossPerBitList object at 0x7f9885227460>[[0.0,None][-2.1213459149294067e-06,None][-8.485383204970276e-07,None][-1.2728074807455414e-06,None][-1.6970766409940552e-06,None][-1.2728074807455414e-06,None][-2.545614961491083e-06,None][-1.6970766409940552e-06,None][-1.0606729006212845e-07,None][-3.1820187018638535e-07,None][-2.121345801242569e-07,None][-2.121345801242569e-07,None][-2.121345801242569e-07,None][0.0,None][-1.0606729006212845e-07,None][-1.0606729006212845e-07,None][-1.0606729006212845e-07,None][0.0,None][-7.955046754659634e-08,None][-4.582107067108154e-06,0.0]]\n",
      "------Fine tune the model------\n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9611 - train_loss: 0.1290 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9651 - train_loss: 0.1042 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9639 - train_loss: 0.1123 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9635 - train_loss: 0.1067 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9639 - train_loss: 0.0997 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9734 - train_loss: 0.0858 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9579 - train_loss: 0.1256 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9639 - train_loss: 0.1097 - val_acc: NA    - val_loss: NA   \n",
      "!!Loss seach end !!\n",
      "=========Finial Tunne=========\n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9736 - train_loss: 0.0830 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9780 - train_loss: 0.0735 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9647 - train_loss: 0.1029 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9617 - train_loss: 0.1111 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9704 - train_loss: 0.0927 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9710 - train_loss: 0.0858 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9698 - train_loss: 0.0999 - val_acc: NA    - val_loss: NA   \n",
      "Epoch: 0/1\n",
      "  156/1531 |==***********************|  - train_acc: 0.9683 - train_loss: 0.0932 - val_acc: NA    - val_loss: NA   \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<qunatize_resnet20.Resnet20 at 0x7f98207ef490>,\n",
       " <quantize_complier.WlflList at 0x7f9884ce6580>)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "q_complier = qc.QunatizeComplier(model_q,full_model,q_layers,train_fn,test_fn,verbose=True,target_compression=0.12,max_loss=0.005)\n",
    "q_complier.quantize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: complier_tmp/3deep_compressed_resnet20_model/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_save_path = 'complier_tmp/3deep_compressed_resnet20_model'\n",
    "wlfl_save_path = 'complier_tmp/3deep_compressed_resnet20_wlfl.txt'\n",
    "model_q.model.save_weights(model_save_path)\n",
    "tf.saved_model.save(model_q.model,model_save_path)\n",
    "q_complier.wl_agent.get_wl_list().save(wlfl_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 1.9256057739257812,Test Accuracy: 66.50999450683594\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.2457000476996117, 2109844)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "\n",
    "model_q.model.load_weights(model_save_path)\n",
    "q_layers = model_q.get_quantizable_layers()\n",
    "wlfl_list = qc.WlflList(len(q_layers))\n",
    "wlfl_list.load(wlfl_save_path)\n",
    "q_complier.fl_agent.apply_wlfl_to_layers(q_layers,wlfl_list)\n",
    "solver = Solver(model_q,data_set,train_options)\n",
    "solver.test_model(X_test,y_test)\n",
    "q_complier.wl_agent.get_compression_rate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 1.9256057739257812,Test Accuracy: 66.50999450683594\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.9256058>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.6651>)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "solver.test_model(X_test,y_test)"
   ]
  }
 ]
}