{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd01e176acbd42c1e3e7f101af43ec62a5c4debfebb92accde047567277d89b2ce2",
   "display_name": "Python 3.8.10 64-bit ('tf-gpu': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import gzip, shutil\n",
    "import os \n",
    "import data_utils as du \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "import numpy as np \n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib.pyplot import imread\n",
    "import matplotlib.pyplot as plt \n",
    "import platform\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15.0, 15.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "Data_folder = 'CIFAR10_Data'\n",
    "Data_fn = 'cifar-10-python'\n",
    "cifar_url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "#Chang to true if need download data \n",
    "Down_load_data = False \n",
    "if Down_load_data:\n",
    "    r = requests.get(cifar_url,allow_redirects=True)\n",
    "    open(os.path.join(Data_folder,Data_fn+'.tar.gz'),'wb').write(r.content)\n",
    "    #Unzip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "import numpy as np \n",
    "from six.moves import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib.pyplot import imread\n",
    "import platform\n",
    "\n",
    "def load_pickle(f):\n",
    "  version = platform.python_version_tuple()\n",
    "  if version[0] == '2':\n",
    "      return  pickle.load(f)\n",
    "  elif version[0] == '3':\n",
    "      return  pickle.load(f, encoding='latin1')\n",
    "  raise ValueError(\"invalid python version: {}\".format(version))\n",
    "\n",
    "def load_CIFAR_batch(fn):\n",
    "    with open(fn, 'rb') as f:\n",
    "        datadict = load_pickle(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Y = np.array(Y)\n",
    "        return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "    \"\"\" load all of cifar \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000,subtract_mean=True):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for training  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'CIFAR10_Data/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    \n",
    "    ## Data post_process \n",
    "   \n",
    "    if subtract_mean:\n",
    "        mean_image = np.mean(X_train, axis=0)\n",
    "        X_train -= mean_image\n",
    "        X_val -= mean_image\n",
    "        X_test -= mean_image\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\nTrain labels shape:  (49000,) int64\nValidation data shape:  (1000, 32, 32, 3)\nValidation labels shape:  (1000,)\nTest data shape:  (10000, 32, 32, 3)\nTest labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data(subtract_mean =True)\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 32, 32, 16)   432         input_1[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n__________________________________________________________________________________________________\nre_lu (ReLU)                    (None, 32, 32, 16)   0           batch_normalization[0][0]        \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 32, 32, 16)   2304        re_lu[0][0]                      \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nre_lu_1 (ReLU)                  (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 32, 32, 16)   2304        re_lu_1[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n__________________________________________________________________________________________________\ntf.__operators__.add (TFOpLambd (None, 32, 32, 16)   0           re_lu[0][0]                      \n                                                                 batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\nre_lu_2 (ReLU)                  (None, 32, 32, 16)   0           tf.__operators__.add[0][0]       \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 32, 32, 16)   2304        re_lu_2[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nre_lu_3 (ReLU)                  (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 32, 32, 16)   2304        re_lu_3[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n__________________________________________________________________________________________________\ntf.__operators__.add_1 (TFOpLam (None, 32, 32, 16)   0           re_lu_2[0][0]                    \n                                                                 batch_normalization_4[0][0]      \n__________________________________________________________________________________________________\nre_lu_4 (ReLU)                  (None, 32, 32, 16)   0           tf.__operators__.add_1[0][0]     \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 32, 32, 16)   2304        re_lu_4[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nre_lu_5 (ReLU)                  (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 32, 32, 16)   2304        re_lu_5[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n__________________________________________________________________________________________________\ntf.__operators__.add_2 (TFOpLam (None, 32, 32, 16)   0           re_lu_4[0][0]                    \n                                                                 batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\nre_lu_6 (ReLU)                  (None, 32, 32, 16)   0           tf.__operators__.add_2[0][0]     \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 16, 16, 32)   4608        re_lu_6[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_7 (BatchNor (None, 16, 16, 32)   128         conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nre_lu_7 (ReLU)                  (None, 16, 16, 32)   0           batch_normalization_7[0][0]      \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 16, 16, 16)   0           re_lu_6[0][0]                    \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 16, 16, 32)   9216        re_lu_7[0][0]                    \n__________________________________________________________________________________________________\ntf.compat.v1.pad (TFOpLambda)   (None, 16, 16, 32)   0           max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n__________________________________________________________________________________________________\ntf.__operators__.add_3 (TFOpLam (None, 16, 16, 32)   0           tf.compat.v1.pad[0][0]           \n                                                                 batch_normalization_8[0][0]      \n__________________________________________________________________________________________________\nre_lu_8 (ReLU)                  (None, 16, 16, 32)   0           tf.__operators__.add_3[0][0]     \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 16, 16, 32)   9216        re_lu_8[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nre_lu_9 (ReLU)                  (None, 16, 16, 32)   0           batch_normalization_9[0][0]      \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 16, 16, 32)   9216        re_lu_9[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_10[0][0]                  \n__________________________________________________________________________________________________\ntf.__operators__.add_4 (TFOpLam (None, 16, 16, 32)   0           re_lu_8[0][0]                    \n                                                                 batch_normalization_10[0][0]     \n__________________________________________________________________________________________________\nre_lu_10 (ReLU)                 (None, 16, 16, 32)   0           tf.__operators__.add_4[0][0]     \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 16, 16, 32)   9216        re_lu_10[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nre_lu_11 (ReLU)                 (None, 16, 16, 32)   0           batch_normalization_11[0][0]     \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 16, 16, 32)   9216        re_lu_11[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n__________________________________________________________________________________________________\ntf.__operators__.add_5 (TFOpLam (None, 16, 16, 32)   0           re_lu_10[0][0]                   \n                                                                 batch_normalization_12[0][0]     \n__________________________________________________________________________________________________\nre_lu_12 (ReLU)                 (None, 16, 16, 32)   0           tf.__operators__.add_5[0][0]     \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 8, 8, 64)     18432       re_lu_12[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_13 (BatchNo (None, 8, 8, 64)     256         conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nre_lu_13 (ReLU)                 (None, 8, 8, 64)     0           batch_normalization_13[0][0]     \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 32)     0           re_lu_12[0][0]                   \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 8, 8, 64)     36864       re_lu_13[0][0]                   \n__________________________________________________________________________________________________\ntf.compat.v1.pad_1 (TFOpLambda) (None, 8, 8, 64)     0           max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nbatch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_14[0][0]                  \n__________________________________________________________________________________________________\ntf.__operators__.add_6 (TFOpLam (None, 8, 8, 64)     0           tf.compat.v1.pad_1[0][0]         \n                                                                 batch_normalization_14[0][0]     \n__________________________________________________________________________________________________\nre_lu_14 (ReLU)                 (None, 8, 8, 64)     0           tf.__operators__.add_6[0][0]     \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 8, 8, 64)     36864       re_lu_14[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nre_lu_15 (ReLU)                 (None, 8, 8, 64)     0           batch_normalization_15[0][0]     \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 8, 8, 64)     36864       re_lu_15[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n__________________________________________________________________________________________________\ntf.__operators__.add_7 (TFOpLam (None, 8, 8, 64)     0           re_lu_14[0][0]                   \n                                                                 batch_normalization_16[0][0]     \n__________________________________________________________________________________________________\nre_lu_16 (ReLU)                 (None, 8, 8, 64)     0           tf.__operators__.add_7[0][0]     \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 8, 8, 64)     36864       re_lu_16[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_17[0][0]                  \n__________________________________________________________________________________________________\nre_lu_17 (ReLU)                 (None, 8, 8, 64)     0           batch_normalization_17[0][0]     \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 8, 8, 64)     36864       re_lu_17[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n__________________________________________________________________________________________________\ntf.__operators__.add_8 (TFOpLam (None, 8, 8, 64)     0           re_lu_16[0][0]                   \n                                                                 batch_normalization_18[0][0]     \n__________________________________________________________________________________________________\nre_lu_18 (ReLU)                 (None, 8, 8, 64)     0           tf.__operators__.add_8[0][0]     \n__________________________________________________________________________________________________\nglobal_average_pooling2d (Globa (None, 64)           0           re_lu_18[0][0]                   \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 10)           650         global_average_pooling2d[0][0]   \n==================================================================================================\nTotal params: 271,098\nTrainable params: 269,722\nNon-trainable params: 1,376\n__________________________________________________________________________________________________\nModel: \"res_net20\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nres_net_encoder (ResNetEncod multiple                  6336      \n_________________________________________________________________\nres_net_decoder (ResNetDecod multiple                  163850    \n=================================================================\nTotal params: 170,186\nTrainable params: 170,186\nNon-trainable params: 0\n_________________________________________________________________\n4\nconv2d_19/kernel:0\nconv2d_19/bias:0\nres_net_decoder/dense_1/kernel:0\nres_net_decoder/dense_1/bias:0\n<tensorflow.python.keras.layers.advanced_activations.ReLU object at 0x7f91940c4c40>\nfdsf\n"
     ]
    }
   ],
   "source": [
    "from solver import Solver\n",
    "from cnn_model import *\n",
    "import resent2 \n",
    "import resnet\n",
    "#model = ThreeLayerConvNet(12, 8, 10)\n",
    "#model = Lennet5(in_channel=3, out_channel=10)\n",
    "#\n",
    "model = resent2.cifar_resnet20('preactivated', shortcut_type='B')\n",
    "#model = resnet.ResNet20(3,10)\n",
    "data_set = {\n",
    "    'train_data': X_train,\n",
    "    'train_label': y_train,\n",
    "    'val_data':X_val,\n",
    "    'val_label':y_val,\n",
    "}\n",
    "train_options = {\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 0.001,\n",
    "    'epoch_num': 20,\n",
    "    'save_dir': 'complier_tmp',\n",
    "    'from_logits': True\n",
    "}\n",
    "\n",
    "#model.build((1,32,32,3))\n",
    "#print(model.summary())\n",
    "solver= Solver(model,data_set,options=train_options)\n",
    "#solver.train()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## load model "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss: 1.0970335006713867,Test Accuracy: 65.97000122070312\n",
      "Loss: 1.0970335006713867,Test Accuracy: 65.97000122070312\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "from solver import Solver\n",
    "import quantize_util as qu \n",
    "model_path = 'complier_tmp/trained_15'\n",
    "model = hub.KerasLayer(model_path,trainable=True)\n",
    "solver= Solver(model,data_set,options=train_options)\n",
    "test_loss, test_acc = solver.test_model(X_test,y_test)\n",
    "model_q = resent2.cifar_resnet20('preactivated', shortcut_type='B')\n",
    "qu.copy_weight(model_q,model)\n",
    "solver= Solver(model_q,data_set,options=train_options)\n",
    "test_loss, test_acc = solver.test_model(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "conv2d_42/kernel:0\nbatch_normalization_39/gamma:0\nbatch_normalization_39/beta:0\nconv2d_43/kernel:0\nbatch_normalization_40/gamma:0\nbatch_normalization_40/beta:0\nconv2d_44/kernel:0\nbatch_normalization_41/gamma:0\nbatch_normalization_41/beta:0\nconv2d_45/kernel:0\nbatch_normalization_42/gamma:0\nbatch_normalization_42/beta:0\nconv2d_46/kernel:0\nbatch_normalization_43/gamma:0\nbatch_normalization_43/beta:0\nconv2d_47/kernel:0\nbatch_normalization_44/gamma:0\nbatch_normalization_44/beta:0\nconv2d_48/kernel:0\nbatch_normalization_45/gamma:0\nbatch_normalization_45/beta:0\nconv2d_49/kernel:0\nbatch_normalization_46/gamma:0\nbatch_normalization_46/beta:0\nconv2d_51/kernel:0\nconv2d_50/kernel:0\nbatch_normalization_47/gamma:0\nbatch_normalization_47/beta:0\nconv2d_52/kernel:0\nbatch_normalization_48/gamma:0\nbatch_normalization_48/beta:0\nconv2d_53/kernel:0\nbatch_normalization_49/gamma:0\nbatch_normalization_49/beta:0\nconv2d_54/kernel:0\nbatch_normalization_50/gamma:0\nbatch_normalization_50/beta:0\nconv2d_55/kernel:0\nbatch_normalization_51/gamma:0\nbatch_normalization_51/beta:0\nconv2d_56/kernel:0\nbatch_normalization_52/gamma:0\nbatch_normalization_52/beta:0\nconv2d_58/kernel:0\nconv2d_57/kernel:0\nbatch_normalization_53/gamma:0\nbatch_normalization_53/beta:0\nconv2d_59/kernel:0\nbatch_normalization_54/gamma:0\nbatch_normalization_54/beta:0\nconv2d_60/kernel:0\nbatch_normalization_55/gamma:0\nbatch_normalization_55/beta:0\nconv2d_61/kernel:0\nbatch_normalization_56/gamma:0\nbatch_normalization_56/beta:0\nconv2d_62/kernel:0\nbatch_normalization_57/gamma:0\nbatch_normalization_57/beta:0\ndense_3/kernel:0\ndense_3/bias:0\n"
     ]
    }
   ],
   "source": [
    "for var in model_q.trainable_variables:\n",
    "    print(var.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(8.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import quantize_util as qu \n",
    "val = qu.to_fixpoint(4.2,wl=8,fl=-3)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import quantize_complier as qc \n",
    "fl_agent = qc.FlSerachAgent(model,solver,[X_test,y_test])\n",
    "test_weight =  model_q.trainable_variables[1].numpy().reshape(-1)\n",
    "min = -32\n",
    "fls = [*range(min,32)]\n",
    "for i in range(1,32):\n",
    "    errors = fl_agent.quantize_abs_mean_errors(test_weight,i,fls)\n",
    "    #print('min error at: {}'.format(np.argmin(errors)+min))\n",
    "    #print(np.min(errors))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n0\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n26\n27\n27\n28\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,32):\n",
    "    best_fl = fl_agent.search_fl(test_weight,i)\n",
    "    print(best_fl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tune the model\n"
     ]
    }
   ],
   "source": [
    "solver = Solver(model,)\n",
    "def tunner():\n",
    "    \n",
    "    print(\"Tune the model\")\n",
    "fl_agent = qc.FlSerachAgent(model,tunner,1)\n",
    "fl_agent.tunne_model()\n"
   ]
  },
  {
   "source": [
    "## Test for FL_agent function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0/1\n",
      " 1250/1250 |=========================|  - train_acc: 0.7416 - train_loss: 0.7569 - val_acc: 0.6960 - val_loss: 0.9562\n",
      "Loss: 1.0654171705245972,Test Accuracy: 65.56999969482422\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6557"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "tune_size = 40000\n",
    "X_tune = X_train[0:tune_size]\n",
    "y_tune = y_train[0:tune_size]\n",
    "tune_dataset = {\n",
    "    'batch_size': 32,\n",
    "    'train_data': X_tune,\n",
    "    'train_label': y_tune,\n",
    "    'val_data':X_val,\n",
    "    'val_label':y_val,\n",
    "}\n",
    "tune_options = {\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 0.001,\n",
    "    'epoch_num': 1,\n",
    "    'from_logits': True\n",
    "}\n",
    "\n",
    "tune_solver = Solver(model,tune_dataset,save_model=False,                               options=tune_options,plot_graph=False)\n",
    "def tune_fn():\n",
    "    tune_solver.train()\n",
    "def acc_fn():\n",
    "    loss,acc = tune_solver.test_model(X_test,y_test)\n",
    "    return acc.numpy() \n",
    "\n",
    "fl_agent = qc.FlSerachAgent(model,tune_fn,acc_fn)\n",
    "fl_agent.tunne_model()\n",
    "fl_agent.model_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}