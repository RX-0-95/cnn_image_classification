{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project1_a_for_class.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python376jvsc74a57bd09d8568969991eace1b69baaf76c4223da2a1dc13aded88f60348c0eee6950b3a",
      "display_name": "Python 3.7.6 64-bit ('base': conda)"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM76mKmn8Lsu"
      },
      "source": [
        "Acknowledgement: \n",
        "\n",
        "Tensorflow 2 official tutorial https://www.tensorflow.org/tutorials/images/transfer_learning\n",
        "\n",
        "CS231n course website https://cs231n.github.io/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3AH0UoaCyA2"
      },
      "source": [
        "# Prepare environment\n",
        "\n",
        "Colab is a free tool for experimentation for your project. \n",
        "Colab will disconnect after 12 hours or ~30 min of idling (and you will lose your unsaved data). So let's mount to your Google drive first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY0xt7wfD3mv"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "#import tensorflow_hub as hub\n",
        "import requests \n",
        "import gzip, shutil\n",
        "import os \n",
        "import data_utils as du \n",
        "import tensorflow as tf \n",
        "from tensorflow import keras \n",
        "import numpy as np \n",
        "from six.moves import cPickle as pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib.pyplot import imread\n",
        "import matplotlib.pyplot as plt \n",
        "import platform\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (15.0, 15.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "Data_folder = 'CIFAR10_Data'\n",
        "Data_fn = 'cifar-10-python'\n",
        "cifar_url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
        "#Chang to true if need download data \n",
        "Down_load_data = False \n",
        "if Down_load_data:\n",
        "    r = requests.get(cifar_url,allow_redirects=True)\n",
        "    open(os.path.join(Data_folder,Data_fn+'.tar.gz'),'wb').write(r.content)\n",
        "    #Unzip the data\n",
        "import tensorflow as tf \n",
        "from tensorflow import keras \n",
        "import numpy as np \n",
        "from six.moves import cPickle as pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib.pyplot import imread\n",
        "import platform\n",
        "\n",
        "def load_pickle(f):\n",
        "  version = platform.python_version_tuple()\n",
        "  if version[0] == '2':\n",
        "      return  pickle.load(f)\n",
        "  elif version[0] == '3':\n",
        "      return  pickle.load(f, encoding='latin1')\n",
        "  raise ValueError(\"invalid python version: {}\".format(version))\n",
        "\n",
        "def load_CIFAR_batch(fn):\n",
        "    with open(fn, 'rb') as f:\n",
        "        datadict = load_pickle(f)\n",
        "        X = datadict['data']\n",
        "        Y = datadict['labels']\n",
        "        X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
        "        Y = np.array(Y)\n",
        "        return X, Y\n",
        "\n",
        "def load_CIFAR10(ROOT):\n",
        "    \"\"\" load all of cifar \"\"\"\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for b in range(1,6):\n",
        "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
        "        X, Y = load_CIFAR_batch(f)\n",
        "        xs.append(X)\n",
        "        ys.append(Y)\n",
        "    Xtr = np.concatenate(xs)\n",
        "    Ytr = np.concatenate(ys)\n",
        "    del X, Y\n",
        "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
        "    return Xtr, Ytr, Xte, Yte\n",
        "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000,subtract_mean=True):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for training  \n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    cifar10_dir = 'CIFAR10_Data/cifar-10-batches-py'\n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "    # Subsample the data\n",
        "    mask = range(num_training, num_training + num_validation)\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = range(num_training)\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = range(num_test)\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "    \n",
        "    ## Data post_process \n",
        "    \n",
        "    if subtract_mean:\n",
        "        mean_image = np.mean(X_train, axis=0)\n",
        "        X_train -= mean_image\n",
        "        X_val -= mean_image\n",
        "        X_test -= mean_image\n",
        "    \n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Train data shape:  (49000, 32, 32, 3)\n",
            "Train labels shape:  (49000,) int64\n",
            "Validation data shape:  (1000, 32, 32, 3)\n",
            "Validation labels shape:  (1000,)\n",
            "Test data shape:  (10000, 32, 32, 3)\n",
            "Test labels shape:  (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YCqgC29H4Lp"
      },
      "source": [
        "# Colab users\n",
        "\n",
        "If you are using Colab, you can manually switch to a GPU device in tf.device(device) scope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uidB0IJFIfx0",
        "outputId": "132070c5-def1-48e0-9238-5b6976eb172a"
      },
      "source": [
        "# Set up some global variables\n",
        "USE_GPU = True\n",
        "\n",
        "if USE_GPU:\n",
        "  device = '/device:GPU:0'\n",
        "else:\n",
        "  device = '/cpu:0'\n",
        "\n",
        "print('Using device: ', device)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device:  /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBJUh6uAFmyO"
      },
      "source": [
        "# Data\n",
        "\n",
        "Load CIFAR-10 dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JiUUdqRHbR6",
        "outputId": "e2d76508-dd62-4318-9f7f-4559e4f7c5b1"
      },
      "source": [
        "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape:  (49000, 32, 32, 3)\nTrain labels shape:  (49000,) int64\nValidation data shape:  (1000, 32, 32, 3)\nValidation labels shape:  (1000,)\nTest data shape:  (10000, 32, 32, 3)\nTest labels shape:  (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnM7ki4FIPiv"
      },
      "source": [
        "class Dataset(object):\n",
        "  def __init__(self, X, y, batch_size, shuffle=False):\n",
        "    \"\"\"\n",
        "    Construct a Dataset object to iterate over data X and labels y\n",
        "    \n",
        "    Inputs:\n",
        "    - X: Numpy array of data, of any shape\n",
        "    - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
        "    - batch_size: Integer giving number of elements per minibatch\n",
        "    - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
        "    \"\"\"\n",
        "    assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
        "    self.X, self.y = X, y\n",
        "    self.batch_size, self.shuffle = batch_size, shuffle\n",
        "\n",
        "  def __iter__(self):\n",
        "    N, B = self.X.shape[0], self.batch_size\n",
        "    idxs = np.arange(N)\n",
        "    if self.shuffle:\n",
        "      np.random.shuffle(idxs)\n",
        "    return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
        "\n",
        "\n",
        "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
        "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
        "test_dset = Dataset(X_test, y_test, batch_size=64)"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLRachAjIWnk",
        "outputId": "5fdf6ab0-5f18-4a5a-9dc3-e4d21d889be5"
      },
      "source": [
        "# We can iterate through a dataset like this:\n",
        "for t, (x, y) in enumerate(train_dset):\n",
        "    print(t, x.shape, y.shape)\n",
        "    if t > 5: break"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 (64, 32, 32, 3) (64,)\n1 (64, 32, 32, 3) (64,)\n2 (64, 32, 32, 3) (64,)\n3 (64, 32, 32, 3) (64,)\n4 (64, 32, 32, 3) (64,)\n5 (64, 32, 32, 3) (64,)\n6 (64, 32, 32, 3) (64,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU0mbpWdJsgD"
      },
      "source": [
        "# Model\n",
        "\n",
        "There are several ways to define the model. We use the low-level APIs in the example model below for flexibility. Check https://www.tensorflow.org/tutorials/images/classification for Sequential/Functional APIs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw2VZYJlKqe1"
      },
      "source": [
        "class ThreeLayerConvNet(tf.keras.Model):\n",
        "  def __init__(self, channel_1, channel_2, num_classes):\n",
        "    super(ThreeLayerConvNet, self).__init__()\n",
        "\n",
        "    initializer = tf.initializers.VarianceScaling(scale=0.002)\n",
        "    self.conv1 = tf.keras.layers.Conv2D(channel_1, [5,5], [1,1], padding='valid',\n",
        "                                     kernel_initializer=initializer,\n",
        "                                     activation=tf.nn.relu)\n",
        "    self.conv2 = tf.keras.layers.Conv2D(channel_2, [3,3], [1,1], padding='valid',\n",
        "                                     kernel_initializer=initializer,\n",
        "                                     activation=tf.nn.relu)\n",
        "    self.fc = tf.keras.layers.Dense(num_classes, kernel_initializer=initializer)\n",
        "    self.flatten = tf.keras.layers.Flatten()   \n",
        "    self.softmax = tf.keras.layers.Softmax()     \n",
        "    \n",
        "  def call(self, x, training=False):\n",
        "    scores = None\n",
        "\n",
        "    padding = tf.constant([[0,0],[2,2],[2,2],[0,0]])\n",
        "    x = tf.pad(x, padding, 'CONSTANT')\n",
        "    x = self.conv1(x)\n",
        "    padding = tf.constant([[0,0],[1,1],[1,1],[0,0]])\n",
        "    x = tf.pad(x, padding, 'CONSTANT')\n",
        "    x = self.conv2(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc(x)\n",
        "    scores = self.softmax(x)\n",
        "       \n",
        "    return scores"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjgmIqOLJt3A"
      },
      "source": [
        "def model_init_fn():\n",
        "  ###########################################################################\n",
        "  # TODO: Implement your model                     #\n",
        "  ###########################################################################\n",
        "  \n",
        "  channel_1, channel_2, num_classes = 12, 8, 10\n",
        "  model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
        "\n",
        "  ###########################################################################\n",
        "  #             END OF YOUR CODE              #\n",
        "  ###########################################################################\n",
        "  return model"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vmRCcNyKceL"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAsk7o2MKeQv"
      },
      "source": [
        "def optimizer_init_fn():\n",
        "  ###########################################################################\n",
        "  # TODO: Implement your optimizer                   #\n",
        "  ###########################################################################\n",
        "  \n",
        "  opt = tf.optimizers.SGD(learning_rate=0.001)\n",
        "\n",
        "  ###########################################################################\n",
        "  #             END OF YOUR CODE              #\n",
        "  ###########################################################################\n",
        "  return opt"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9OfirMWMxaC"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-or6sR2MyaU"
      },
      "source": [
        "def loss_init_fn():\n",
        "  ############################################################################\n",
        "  # TODO: Implement your loss                      #\n",
        "  ############################################################################\n",
        "  \n",
        "  loss = keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "  ###########################################################################\n",
        "  #             END OF YOUR CODE              #\n",
        "  ###########################################################################\n",
        "  return loss"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsCNnyEQJuTE"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m5AYm4JJvYO"
      },
      "source": [
        "def train(model, optimizer, loss_fn, num_epochs=1, save_every_num_epoch=2, save_dir='tmp/', save_model_name='trained', is_training=False):\n",
        "  \"\"\"\n",
        "  Simple training loop for use with models defined using tf.keras. It trains\n",
        "  a model for one epoch on the CIFAR-10 training set and periodically checks\n",
        "  accuracy on the CIFAR-10 validation set.\n",
        "  \n",
        "  Inputs:\n",
        "  - num_epochs: The number of epochs to train for\n",
        "  - save_every_num_epoch: The number of epochs to export the trained model\n",
        "  - save_dir: Directory to export the trained model\n",
        "  - save_model_name: Sub-directory to export the trained model (also the name to reload the model)\n",
        "  \n",
        "  Returns: Nothing, but prints progress during training\n",
        "  \"\"\"    \n",
        "  with tf.device(device):\n",
        "   \n",
        "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "    val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
        "    val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
        "    \n",
        "    t = 0\n",
        "    for epoch in range(num_epochs):      \n",
        "      # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
        "      train_loss.reset_states()\n",
        "      train_accuracy.reset_states()\n",
        "      \n",
        "      for x_np, y_np in train_dset:\n",
        "        with tf.GradientTape() as tape:\n",
        "          ###########################################################################\n",
        "          # TODO: 1. Build forward pass with model             #    \n",
        "          #     2. Compute the loss with loss_fn             #\n",
        "          ###########################################################################\n",
        "          \n",
        "          scores = model(x_np)\n",
        "          #print(scores)\n",
        "          loss = loss_fn(y_np,scores)\n",
        "\n",
        "          ###########################################################################\n",
        "          #             END OF YOUR CODE              #\n",
        "          ###########################################################################\n",
        "\n",
        "          gradients = tape.gradient(loss, model.trainable_variables)\n",
        "          optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "          \n",
        "          # Update the metrics\n",
        "          train_loss.update_state(loss)\n",
        "          train_accuracy.update_state(scores,y_np)\n",
        "          \n",
        "          if t % 100 == 0:\n",
        "            val_loss.reset_states()\n",
        "            val_accuracy.reset_states()\n",
        "            for test_x, test_y in val_dset:\n",
        "              ###########################################################################\n",
        "              # TODO: 1. Run validation with model                #    \n",
        "              #     2. Compute the loss with loss_fn             #\n",
        "              ###########################################################################\n",
        "              \n",
        "              prediction = model(test_x)\n",
        "              t_loss = loss_fn(test_y,prediction)\n",
        "\n",
        "              ###########################################################################\n",
        "              #             END OF YOUR CODE              #\n",
        "              ###########################################################################\n",
        "\n",
        "              val_loss.update_state(t_loss)\n",
        "              val_accuracy.update_state(test_y, prediction)\n",
        "            \n",
        "            template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'\n",
        "            print (template.format(t, epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result()*100,\n",
        "                        val_loss.result(),\n",
        "                        val_accuracy.result()*100))\n",
        "          t += 1\n",
        "      if epoch % save_every_num_epoch or epoch == num_epochs - 1:\n",
        "        if not os.path.exists(save_dir):\n",
        "          os.makedirs(save_dir)\n",
        "        save_path = os.path.join(save_dir, save_model_name+'-'+str(epoch+1))\n",
        "        tf.saved_model.save(model, save_path)"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHhrjGH0K0tf",
        "outputId": "709b24b4-5d38-4efd-c9ce-4c2c3f789236",
        "tags": []
      },
      "source": [
        "model = model_init_fn()\n",
        "optimizer = optimizer_init_fn()\n",
        "loss = loss_init_fn()\n",
        "train(model, optimizer, loss)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Epoch 1, Loss: 2.3024110794067383, Accuracy: 0.0, Val Loss: 2.3026750087738037, Val Accuracy: 11.59999942779541\n",
            "Iteration 100, Epoch 1, Loss: 2.2164223194122314, Accuracy: 0.0, Val Loss: 2.1063730716705322, Val Accuracy: 26.80000114440918\n",
            "Iteration 200, Epoch 1, Loss: 2.1237564086914062, Accuracy: 0.0, Val Loss: 1.9352222681045532, Val Accuracy: 32.20000076293945\n",
            "Iteration 300, Epoch 1, Loss: 2.056257486343384, Accuracy: 0.0, Val Loss: 1.8988291025161743, Val Accuracy: 33.70000076293945\n",
            "Iteration 400, Epoch 1, Loss: 1.9927791357040405, Accuracy: 0.0, Val Loss: 1.8013007640838623, Val Accuracy: 36.599998474121094\n",
            "Iteration 500, Epoch 1, Loss: 1.9510821104049683, Accuracy: 0.0, Val Loss: 1.7409687042236328, Val Accuracy: 38.400001525878906\n",
            "Iteration 600, Epoch 1, Loss: 1.9156017303466797, Accuracy: 0.0, Val Loss: 1.6906319856643677, Val Accuracy: 40.900001525878906\n",
            "Iteration 700, Epoch 1, Loss: 1.8844273090362549, Accuracy: 0.0, Val Loss: 1.6449127197265625, Val Accuracy: 41.20000076293945\n",
            "INFO:tensorflow:Assets written to: tmp/trained-1/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKrCJ1nQWs9X"
      },
      "source": [
        "# Finetune\n",
        "\n",
        "https://www.tensorflow.org/hub/tf2_saved_model?hl=en"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwZe3jyoWui6",
        "outputId": "fd9e7fc2-e0ee-4ff8-e708-92a0fc1c0f97"
      },
      "source": [
        "model = hub.KerasLayer(\"tmp/trained-3\", trainable = True)\n",
        "# redefine the optimizer (we usually use smaller learning rate for finetuning)\n",
        "train(model, optimizer, loss, num_epochs=1)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hub' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-169-173747f214e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tmp/trained-3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# redefine the optimizer (we usually use smaller learning rate for finetuning)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hub' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ9RZ1yFY5lU"
      },
      "source": [
        "# Quantize a pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC4t3lLHY4p5"
      },
      "source": [
        "model = hub.KerasLayer(\"tmp/trained-3\", trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IgqSVyccivx"
      },
      "source": [
        "def my_quantize_strategy(var):\n",
        "  ############################################################################\n",
        "  # TODO: Implement your quantization strategy             #\n",
        "  ############################################################################\n",
        "  \n",
        "  var = -var\n",
        "\n",
        "  ###########################################################################\n",
        "  #             END OF YOUR CODE              #\n",
        "  ###########################################################################\n",
        "  return var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MDJES_kaJNx"
      },
      "source": [
        "for var in model.trainable_variables:\n",
        "  var.assign(my_quantize_strategy(var))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}