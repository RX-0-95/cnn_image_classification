{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd09d8568969991eace1b69baaf76c4223da2a1dc13aded88f60348c0eee6950b3a",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'true' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-59e43715d294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;34m\"provenance\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;34m\"collapsed_sections\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0;34m\"toc_visible\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     },\n\u001b[1;32m     11\u001b[0m     \"kernelspec\": {\n",
      "\u001b[0;31mNameError\u001b[0m: name 'true' is not defined"
     ]
    }
   ],
   "source": [
    "{\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 0,\n",
    "  \"metadata\": {\n",
    "    \"colab\": {\n",
    "      \"name\": \"project1_b_for_class.ipynb\",\n",
    "      \"provenance\": [],\n",
    "      \"collapsed_sections\": [],\n",
    "      \"toc_visible\": true\n",
    "    },\n",
    "    \"kernelspec\": {\n",
    "      \"display_name\": \"Python 3\",\n",
    "      \"name\": \"python3\"\n",
    "    }\n",
    "  },\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"YOGMXh8jgGu0\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"### Train\\n\",\n",
    "        \"\\n\",\n",
    "        \"save_model argument to control if we export model to disk during training \"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"mldAAr1Tf117\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"def train(model, optimizer, loss_fn, num_epochs=1, save_model=True, save_every_num_epoch=2, save_dir='tmp/', save_model_name='trained', is_training=False):\\n\",\n",
    "        \"  \\\"\\\"\\\"\\n\",\n",
    "        \"  Simple training loop for use with models defined using tf.keras. It trains\\n\",\n",
    "        \"  a model for one epoch on the CIFAR-10 training set and periodically checks\\n\",\n",
    "        \"  accuracy on the CIFAR-10 validation set.\\n\",\n",
    "        \"  \\n\",\n",
    "        \"  Inputs:\\n\",\n",
    "        \"  - num_epochs: The number of epochs to train for\\n\",\n",
    "        \"  - save_every_num_epoch: The number of epochs to export the trained model\\n\",\n",
    "        \"  - save_dir: Directory to export the trained model\\n\",\n",
    "        \"  - save_model_name: Sub-directory to export the trained model (also the name to reload the model)\\n\",\n",
    "        \"  \\n\",\n",
    "        \"  Returns: Nothing, but prints progress during training\\n\",\n",
    "        \"  \\\"\\\"\\\"    \\n\",\n",
    "        \"  with tf.device(device):\\n\",\n",
    "        \"   \\n\",\n",
    "        \"    train_loss = tf.keras.metrics.Mean(name='train_loss')\\n\",\n",
    "        \"    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\\n\",\n",
    "        \"\\n\",\n",
    "        \"    val_loss = tf.keras.metrics.Mean(name='val_loss')\\n\",\n",
    "        \"    val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    t = 0\\n\",\n",
    "        \"    for epoch in range(num_epochs):      \\n\",\n",
    "        \"      # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\\n\",\n",
    "        \"      train_loss.reset_states()\\n\",\n",
    "        \"      train_accuracy.reset_states()\\n\",\n",
    "        \"      \\n\",\n",
    "        \"      for x_np, y_np in train_dset:\\n\",\n",
    "        \"        with tf.GradientTape() as tape:\\n\",\n",
    "        \"          ###########################################################################\\n\",\n",
    "        \"          # TODO: 1. Build forward pass with model             #    \\n\",\n",
    "        \"          #     2. Compute the loss with loss_fn             #\\n\",\n",
    "        \"          ###########################################################################\\n\",\n",
    "        \"          \\n\",\n",
    "        \"          scores = \\n\",\n",
    "        \"          loss = \\n\",\n",
    "        \"\\n\",\n",
    "        \"          ###########################################################################\\n\",\n",
    "        \"          #             END OF YOUR CODE              #\\n\",\n",
    "        \"          ###########################################################################\\n\",\n",
    "        \"\\n\",\n",
    "        \"          gradients = tape.gradient(loss, model.trainable_variables)\\n\",\n",
    "        \"          optimizer.apply_gradients(zip(gradients, model.trainable_variables))\\n\",\n",
    "        \"          \\n\",\n",
    "        \"          # Update the metrics\\n\",\n",
    "        \"          train_loss.update_state(loss)\\n\",\n",
    "        \"          train_accuracy.update_state(y_np, scores)\\n\",\n",
    "        \"          \\n\",\n",
    "        \"          if t % 100 == 0:\\n\",\n",
    "        \"            val_loss.reset_states()\\n\",\n",
    "        \"            val_accuracy.reset_states()\\n\",\n",
    "        \"            for test_x, test_y in val_dset:\\n\",\n",
    "        \"              ###########################################################################\\n\",\n",
    "        \"              # TODO: 1. Run validation with model                #    \\n\",\n",
    "        \"              #     2. Compute the loss with loss_fn             #\\n\",\n",
    "        \"              ###########################################################################\\n\",\n",
    "        \"              \\n\",\n",
    "        \"              prediction = \\n\",\n",
    "        \"              t_loss = \\n\",\n",
    "        \"\\n\",\n",
    "        \"              ###########################################################################\\n\",\n",
    "        \"              #             END OF YOUR CODE              #\\n\",\n",
    "        \"              ###########################################################################\\n\",\n",
    "        \"\\n\",\n",
    "        \"              val_loss.update_state(t_loss)\\n\",\n",
    "        \"              val_accuracy.update_state(test_y, prediction)\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'\\n\",\n",
    "        \"            print (template.format(t, epoch+1,\\n\",\n",
    "        \"                        train_loss.result(),\\n\",\n",
    "        \"                        train_accuracy.result()*100,\\n\",\n",
    "        \"                        val_loss.result(),\\n\",\n",
    "        \"                        val_accuracy.result()*100))\\n\",\n",
    "        \"          t += 1\\n\",\n",
    "        \"      if save_model and (epoch % save_every_num_epoch or epoch == num_epochs - 1):\\n\",\n",
    "        \"        if not os.path.exists(save_dir):\\n\",\n",
    "        \"          os.makedirs(save_dir)\\n\",\n",
    "        \"        save_path = os.path.join(save_dir, save_model_name+'-'+str(epoch+1))\\n\",\n",
    "        \"        tf.saved_model.save(model, save_path)\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"vZ9RZ1yFY5lU\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# Quantize a trained model\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"k1M-1L-wdOAM\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"### Implement your quantization formula\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"_FKxpe1caVsG\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"Define a quantization operator. Please use Tensorflow 2.0 API because we need to insert this operator to the model.\\n\",\n",
    "        \"\\n\",\n",
    "        \"\\n\",\n",
    "        \"Hint: tf.clip_by_value() for overflow\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"d4eGrrGbB-h4\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"@tf.custom_gradient\\n\",\n",
    "        \"def to_fixedpoint(value, wl=8, fl=4):\\n\",\n",
    "        \"  ###########################################################################\\n\",\n",
    "        \"  # TODO: Implement the data quantization with Tensorflow 2 API  #\\n\",\n",
    "        \"  ###########################################################################\\n\",\n",
    "        \"  \\n\",\n",
    "        \"  # value_q should be a tensorflow tensor\\n\",\n",
    "        \"  value_q = \\n\",\n",
    "        \"\\n\",\n",
    "        \"  ###########################################################################\\n\",\n",
    "        \"  #             END OF YOUR CODE              #\\n\",\n",
    "        \"  ###########################################################################\\n\",\n",
    "        \"  def grad(upstream):\\n\",\n",
    "        \"    return upstream,0.0,0.0\\n\",\n",
    "        \"  return value_q, grad\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"Jlt_fxl5cO_M\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"Check if your float-to-fixedpoint implmentation works. Pay attention to the conversion between tensorflow.Tensor and numpy.array.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"AQq7I7FnctTo\",\n",
    "        \"outputId\": \"ee3f375c-b3f1-42ad-894a-af302cddb258\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"to_fixedpoint(np.array([1.2,1.2]),wl=8,fl=1).numpy()\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"execute_result\",\n",
    "          \"data\": {\n",
    "            \"text/plain\": [\n",
    "              \"array([1., 1.])\"\n",
    "            ]\n",
    "          },\n",
    "          \"metadata\": {\n",
    "            \"tags\": []\n",
    "          },\n",
    "          \"execution_count\": 104\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"NTiPn_C_gxTV\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"How to determine the word length and fraction length? \\n\",\n",
    "        \"\\n\",\n",
    "        \"We can first fix the word length to be 8 for weight and intermediate feature maps, and 16 for bias. Then we linearly search over limited set of fraction length (e.g.,range(-4,20)) to locate the fraction length leading to least absolute error compared to full-precision parameters.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"um6REcH6jMnJ\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# Example search flow\\n\",\n",
    "        \"errors = []\\n\",\n",
    "        \"for fl in fl_options:\\n\",\n",
    "        \"  data_q = to_fixedpoint(data, wl=8, fl).numpy()\\n\",\n",
    "        \"  error = np.mean(np.abs(data_q - data))\\n\",\n",
    "        \"  errors.append(error)\\n\",\n",
    "        \"fl_best = np.argmin(errors)\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"sQ-9xXkikJ_v\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"You should first extract the data from the model and do the analysis. Find your word length and fraction length. Then apply them to the model as follows.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"ZwMZHkEndXR1\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"### Quantize intermediate feature maps\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"YaO6fB1bamBz\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"Re-define the same model. However, we insert quantization operators for\\n\",\n",
    "        \"intermediate feature map for each layer. Notice that one layer here \\n\",\n",
    "        \"is a fused operator from conv/fc + activation/pooling/residual addition.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"D0-oW3RoAUUm\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"class ThreeLayerConvNet(tf.keras.Model):\\n\",\n",
    "        \"  def __init__(self, channel_1, channel_2, num_classes):\\n\",\n",
    "        \"    super(ThreeLayerConvNet, self).__init__()\\n\",\n",
    "        \"\\n\",\n",
    "        \"    initializer = tf.initializers.VarianceScaling(scale=2.0)\\n\",\n",
    "        \"    self.conv1 = tf.keras.layers.Conv2D(channel_1, [5,5], [1,1], padding='valid',\\n\",\n",
    "        \"                                        kernel_regularizer=l2(0.01),\\n\",\n",
    "        \"                                        kernel_initializer=initializer,\\n\",\n",
    "        \"                                        activation=tf.nn.relu)\\n\",\n",
    "        \"    self.conv2 = tf.keras.layers.Conv2D(channel_2, [3,3], [1,1], padding='valid',\\n\",\n",
    "        \"                                        kernel_initializer=initializer,\\n\",\n",
    "        \"                                        activation=tf.nn.relu)\\n\",\n",
    "        \"    self.fc = tf.keras.layers.Dense(num_classes, kernel_initializer=initializer)\\n\",\n",
    "        \"    self.flatten = tf.keras.layers.Flatten()   \\n\",\n",
    "        \"    self.softmax = tf.keras.layers.Softmax()     \\n\",\n",
    "        \"    \\n\",\n",
    "        \"  def call(self, x, training=False):\\n\",\n",
    "        \"    scores = None\\n\",\n",
    "        \"    x = to_fixedpoint(x, 8, 4)   # <-------- quantize input tensor\\n\",\n",
    "        \"    padding = tf.constant([[0,0],[2,2],[2,2],[0,0]])\\n\",\n",
    "        \"    x = tf.pad(x, padding, 'CONSTANT')\\n\",\n",
    "        \"    x = self.conv1(x)\\n\",\n",
    "        \"    x = to_fixedpoint(x, 8, 4)   # <-------- quantize intermediate feature map after layer 1\\n\",\n",
    "        \"    padding = tf.constant([[0,0],[1,1],[1,1],[0,0]])\\n\",\n",
    "        \"    x = tf.pad(x, padding, 'CONSTANT')\\n\",\n",
    "        \"    x = self.conv2(x)\\n\",\n",
    "        \"    x = to_fixedpoint(x, 8, 4)   # <-------- quantize intermediate feature map after layer 2\\n\",\n",
    "        \"    x = self.flatten(x)\\n\",\n",
    "        \"    x = self.fc(x)\\n\",\n",
    "        \"    x = to_fixedpoint(x, 8, 4)   # <-------- quantize intermediate feature map after layer 3\\n\",\n",
    "        \"    scores = self.softmax(x)\\n\",\n",
    "        \"     \\n\",\n",
    "        \"    return scores\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"WD3TtVgPbyV3\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"Load trained model. Initialize re-defined model with trained variables.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"AC4t3lLHY4p5\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"model = hub.KerasLayer(\\\"tmp/my_model-3\\\", trainable=True)\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"KHTwmQwdYXBg\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"model_q = ThreeLayerConvNet(12,8,10)\\n\",\n",
    "        \"for test_x, test_y in test_dset:\\n\",\n",
    "        \"  prediction = model_q(test_x, training=True)\\n\",\n",
    "        \"  break\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"UlwNaiCcYg5l\",\n",
    "        \"outputId\": \"72641b06-49e5-4264-e9e7-65a5bcfdcc91\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"print('trainable variables in model:')\\n\",\n",
    "        \"for var in model.trainable_variables:\\n\",\n",
    "        \"  print(var.name)\\n\",\n",
    "        \"print('\\\\ntrainable variables in model_q:')\\n\",\n",
    "        \"for var in model_q.trainable_variables:\\n\",\n",
    "        \"  print(var.name)\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"trainable variables in model:\\n\",\n",
    "            \"three_layer_conv_net_12/conv2d_24/kernel:0\\n\",\n",
    "            \"three_layer_conv_net_12/conv2d_24/bias:0\\n\",\n",
    "            \"three_layer_conv_net_12/conv2d_25/kernel:0\\n\",\n",
    "            \"three_layer_conv_net_12/conv2d_25/bias:0\\n\",\n",
    "            \"three_layer_conv_net_12/dense_12/kernel:0\\n\",\n",
    "            \"three_layer_conv_net_12/dense_12/bias:0\\n\",\n",
    "            \"\\n\",\n",
    "            \"trainable variables in model_q:\\n\",\n",
    "            \"three_layer_conv_net_19/conv2d_36/kernel:0\\n\",\n",
    "            \"three_layer_conv_net_19/conv2d_36/bias:0\\n\",\n",
    "            \"three_layer_conv_net_19/conv2d_37/kernel:0\\n\",\n",
    "            \"three_layer_conv_net_19/conv2d_37/bias:0\\n\",\n",
    "            \"three_layer_conv_net_19/dense_18/kernel:0\\n\",\n",
    "            \"three_layer_conv_net_19/dense_18/bias:0\\n\"\n",
    "          ],\n",
    "          \"name\": \"stdout\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"wwLPtC3_Z3Tf\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"for i in range(len(model_q.trainable_variables)):\\n\",\n",
    "        \"  model_q.trainable_variables[i].assign(model.trainable_variables[i])\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"3lCknlBddEFm\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"### Quantize trainable variables (weight & bias)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"tBWw356Edlsw\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"def quantize_weights(model_q):\\n\",\n",
    "        \"  for var in model_q.trainable_variables:\\n\",\n",
    "        \"    var.assign(to_fixedpoint(var, wl=8, fl=4).numpy())\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"6pNMu69whl3o\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"### Workflow\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"X484bxdshsY7\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# Quantize weights and bias\\n\",\n",
    "        \"quantize_weights(model_q)\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"tnJEF00Fh9lk\",\n",
    "        \"outputId\": \"a852fc48-b151-4cee-85ac-f6a7d860c697\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# Check testing accuracy\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"Test Accuracy: 46.5\\n\"\n",
    "          ],\n",
    "          \"name\": \"stdout\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"BZ2r2zCth_2l\",\n",
    "        \"outputId\": \"bf84057e-e55f-4531-901e-50eebfce87f7\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# If accuarcy is not acceptable, we finetune the model\\n\",\n",
    "        \"# use smaller learning rate for finetuning e.g.,1e-4\\n\",\n",
    "        \"train(model_q, optimizer, loss, num_epochs=1, save_model=False)\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"Iteration 0, Epoch 1, Loss: 1.352893590927124, Accuracy: 56.25, Val Loss: 1.4792847633361816, Val Accuracy: 46.79999923706055\\n\",\n",
    "            \"Iteration 100, Epoch 1, Loss: 1.3915162086486816, Accuracy: 50.89727783203125, Val Loss: 1.443684458732605, Val Accuracy: 47.79999923706055\\n\",\n",
    "            \"Iteration 200, Epoch 1, Loss: 1.384037733078003, Accuracy: 51.484764099121094, Val Loss: 1.4255183935165405, Val Accuracy: 49.79999923706055\\n\",\n",
    "            \"Iteration 300, Epoch 1, Loss: 1.378582239151001, Accuracy: 51.47944259643555, Val Loss: 1.4159690141677856, Val Accuracy: 50.5\\n\",\n",
    "            \"Iteration 400, Epoch 1, Loss: 1.3637597560882568, Accuracy: 52.076839447021484, Val Loss: 1.4055781364440918, Val Accuracy: 51.599998474121094\\n\",\n",
    "            \"Iteration 500, Epoch 1, Loss: 1.3526579141616821, Accuracy: 52.60728454589844, Val Loss: 1.4022130966186523, Val Accuracy: 51.400001525878906\\n\",\n",
    "            \"Iteration 600, Epoch 1, Loss: 1.3512036800384521, Accuracy: 52.63103103637695, Val Loss: 1.397850751876831, Val Accuracy: 51.30000305175781\\n\",\n",
    "            \"Iteration 700, Epoch 1, Loss: 1.3441941738128662, Accuracy: 53.03807067871094, Val Loss: 1.3915975093841553, Val Accuracy: 51.89999771118164\\n\"\n",
    "          ],\n",
    "          \"name\": \"stdout\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"p2Ut2exajFM2\",\n",
    "        \"outputId\": \"75a01b94-68e4-4227-a2e2-0a332d04b3ab\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# Check testing accuracy\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"Test Accuracy: 49.91999816894531\\n\"\n",
    "          ],\n",
    "          \"name\": \"stdout\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"37-aIarRjyc5\",\n",
    "        \"outputId\": \"d6660803-fb37-414f-eb8f-b5a9d00f4ce1\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"# Quantize weights and bias (perhaps you can repick the word length/fraction length?)\\n\",\n",
    "        \"quantize_weights(model_q)\\n\",\n",
    "        \"# Check testing accuracy\"\n",
    "      ],\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"text\": [\n",
    "            \"Test Accuracy: 46.5\\n\"\n",
    "          ],\n",
    "          \"name\": \"stdout\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}